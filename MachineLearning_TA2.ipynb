{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MachineLearning_TA2.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOutq3W32zqumU1tIm7CFIY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/E23A48/MachineLearning_LSTM/blob/main/MachineLearning_TA2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8dXq_fLqDd3"
      },
      "source": [
        "**TA2**\n",
        "**MACHINE LEARNING**\n",
        "2020-2\n",
        "\n",
        "Integrantes:\n",
        "*   Piero Herrera\n",
        "*   Camilo Silva\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J5z9cnBKqgu3"
      },
      "source": [
        "## **Introducción y motivación de uso**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X_EfChmurA1M"
      },
      "source": [
        "*Para 1997*, las redes neuronales recurrentes o **recurrent neural networks** – RNN por sus siglas en inglés -, eran utilizadas para diversas aplicaciones tales como el procesamiento de voz o la composición musical, tal y como detallan Hochreiter et. Al en ***“Long Short-Term Memory”***. La principal ventaja de este tipo de red frente a los enfoques tradicionales es el hecho de poder procesar secuencias de datos mediante conexiones de retroalimentación que permitan cierta persistencia de los datos a corto plazo. \n",
        "\n",
        "*Por otro lado*, un problema conocido para la época era el hecho de que los algoritmos existentes para decidir qué poner en la memoria a corto plazo no funcionaban de la forma óptima. Además, almacenar información en intervalos de tiempos prolongados (memoria a largo plazo o ***long-term memory***), era un problema que tomaba demasiado tiempo debido a un error conocido como *flujo de retorno de error insuficiente*, lo que impide que se realice el aprendizaje de secuencias de datos largos y que fue estudiado a profundidad por Hochreiter en 1991 en su tesis de diplomado “Untersuchungen zu dynamischen neuronalen Netzen” (Investigaciones sobre redes neuronales dinámicas). \n",
        "\n",
        "Esto motivó a Sepp Hochreiter y Jürgen Schmidhuber a presentar en 1997 ***“Long Short-Term Memory”***, una arquitectura de red recurrente que lograba solucionar estos errores, principalmente, mediante la definición de un concepto conocido como “***celda de memoria***” que era incorporado a las redes recurrentes tradicionales, dichas celdas pueden mantener su valor durante un periodo de tiempo corto o largo, lo que aporta robustez a la arquitectura, permitiéndole operar sobre distintos tipos de secuencias con diferentes tipos de extensiones con un resultado óptimo, diferenciandolo del enfoque de las RNN.\n",
        "\n",
        "*En la actualidad*, la popularidad de la arquitectura recurrente **LSTM** ha crecido a tal punto de ser utilizado en diferentes tipos de aplicaciones, tales como reconocimiento de escritura o reconocimiento de voz por parte de grandes compañías como, por ejemplo, *IBM* o *Google*.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DewWs4YkqgnF"
      },
      "source": [
        "## **Cómo funciona**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WIkCcm10rBu2"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05aobgnsqgft"
      },
      "source": [
        "## **Bibliotecas**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XDfquQWVrCMz"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6uaSxClqz4K"
      },
      "source": [
        "## **Ejemplos de aplicaciones**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OpXuNMALrC5Y"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OAiNCEqUq3w3"
      },
      "source": [
        "## **Conclusiones**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Nd4McworDVc"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPY7fK1Jq6zb"
      },
      "source": [
        "## **Referencias**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypu0jmZ7rDx7"
      },
      "source": [
        ""
      ]
    }
  ]
}